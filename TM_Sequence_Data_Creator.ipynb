{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#archive:\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, image_filenames, labels, batch_size) :\n",
    "    self.image_filenames = image_filenames\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    \n",
    "    return np.array([\n",
    "            resize(imread('/content/all_images/' + str(file_name)), (80, 80, 3))\n",
    "               for file_name in batch_x])/255.0, np.array(batch_y)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "inputs= [array2[:n],array3[:n],array4[:n],array1_4[:n],array1_5[:n],array1_6[:n]]\n",
    "targets=[array1_1[:n],array1_2[:n],array1_3[:n]]\n",
    "\n",
    "\n",
    "self.paddings=tf.constant([[0,0], [num_steps_out,0], [0,0]])\n",
    "x = tf.slice(tf.pad(x,paddings=self.paddings),[0,0,0],x.shape)\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = [f'datasets/TM_dataset_enc9_part{i}.npz' for i in range(1,6)] \n",
    "batch_size=64\n",
    "counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset part 0 :\n",
      "\tnum samples: 122538\n",
      "\tnum batches: 1914\n",
      "\t100 batches done\n",
      "\t200 batches done\n",
      "\t300 batches done\n",
      "\t400 batches done\n",
      "\t500 batches done\n",
      "\t600 batches done\n",
      "\t700 batches done\n",
      "\t800 batches done\n",
      "\t900 batches done\n",
      "\t1000 batches done\n",
      "\t1100 batches done\n",
      "\t1200 batches done\n",
      "\t1300 batches done\n",
      "\t1400 batches done\n",
      "\t1500 batches done\n",
      "\t1600 batches done\n",
      "\t1700 batches done\n",
      "\t1800 batches done\n",
      "\t1900 batches done\n",
      "dataset part 1 :\n"
     ]
    }
   ],
   "source": [
    "\"\"\"counter = 0\n",
    "for k,dataset_path in enumerate(dataset_paths):\n",
    "    print(f'dataset part {k} :')\n",
    "    try :\n",
    "        print('loading...')\n",
    "        loaded = np.load(dataset_path) \n",
    "        array_drum = loaded['array1_drm']\n",
    "        array_piano = loaded['array2_pia']\n",
    "        array_gtr = loaded['array3_gtr']\n",
    "        array_bass = loaded['array4_bss']\n",
    "\n",
    "        num_samples = array_drum.shape[0]\n",
    "        num_batches = int(num_samples/batch_size)\n",
    "        print(f'\\tnum samples: {num_samples}')\n",
    "        print(f'\\tnum batches: {num_batches}')\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            counter+=1\n",
    "            if counter%100==0:\n",
    "                print(f'\\t{counter} batches done')\n",
    "            path=f'datasets/data_for_sequence/batch_{counter}'\n",
    "\n",
    "            np.savez_compressed(\n",
    "                path,\n",
    "                array1_drm=array_drum[i*batch_size:(i+1)*batch_size],\n",
    "                array2_pia=array_piano[i*batch_size:(i+1)*batch_size],\n",
    "                array3_gtr=array_gtr[i*batch_size:(i+1)*batch_size],\n",
    "                array4_bss=array_bass[i*batch_size:(i+1)*batch_size],\n",
    "            )\n",
    "    except:\n",
    "        print(f'Error! {sys.exc_info()[0]} occurred')\n",
    "        break\n",
    "            \n",
    "print()\n",
    "print(f'{counter} batches created from {counter*batch_size} files')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of files per subdataset:\n",
    "    #part1: 122538 files, 1914 batches\n",
    "    #part2: 116751 files, 1814 batches\n",
    "    #part3: 101364 files, 1583 batches\n",
    "    #part4: ca 109651 files, 1725 batches\n",
    "    #part5: 106954 files, 1671 batches\n",
    "# total batches 8707\n",
    "# total files read 557248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset part 5 :\n",
      "loading...\n",
      "\tnum samples: 106954\n",
      "\tnum batches: 1671\n",
      "\t7100 batches done\n",
      "\t7200 batches done\n",
      "\t7300 batches done\n",
      "\t7400 batches done\n",
      "\t7500 batches done\n",
      "\t7600 batches done\n",
      "\t7700 batches done\n",
      "\t7800 batches done\n",
      "\t7900 batches done\n",
      "\t8000 batches done\n",
      "\t8100 batches done\n",
      "\t8200 batches done\n",
      "\t8300 batches done\n",
      "\t8400 batches done\n",
      "\t8500 batches done\n",
      "\t8600 batches done\n",
      "\t8700 batches done\n",
      "8707 batches created from 557248 files as of now\n"
     ]
    }
   ],
   "source": [
    "counter=7036\n",
    "n=5\n",
    "print(f'dataset part {n} :')\n",
    "print('loading...')\n",
    "dataset_path=f'datasets/TM_dataset_enc9_part{n}.npz'\n",
    "\n",
    "loaded = np.load(dataset_path) \n",
    "array_drum = loaded['array1_drm']\n",
    "array_piano = loaded['array2_pia']\n",
    "array_gtr = loaded['array3_gtr']\n",
    "array_bass = loaded['array4_bss']\n",
    "\n",
    "num_samples = array_drum.shape[0]\n",
    "num_batches = int(num_samples/batch_size)\n",
    "print(f'\\tnum samples: {num_samples}')\n",
    "print(f'\\tnum batches: {num_batches}')\n",
    "\n",
    "for i in range(num_batches):\n",
    "    counter+=1\n",
    "    if counter%100==0:\n",
    "        print(f'\\t{counter} batches done')\n",
    "    path=f'datasets/data_for_sequence/batch_{counter}'\n",
    "\n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        array1_drm=array_drum[i*batch_size:(i+1)*batch_size],\n",
    "        array2_pia=array_piano[i*batch_size:(i+1)*batch_size],\n",
    "        array3_gtr=array_gtr[i*batch_size:(i+1)*batch_size],\n",
    "        array4_bss=array_bass[i*batch_size:(i+1)*batch_size],\n",
    "    )\n",
    "print(f'{counter} batches created from {counter*batch_size} files as of now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loaded\n",
    "del array_drum\n",
    "del array_piano\n",
    "del array_gtr\n",
    "del array_bass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,dataset_path,length,batch_size,pad=1):\n",
    "        self.dataset_path=dataset_path\n",
    "        self.length=length\n",
    "        self.batch_size=batch_size\n",
    "        self.paddings=tf.constant([[0,0], [pad,0], [0,0]])\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self,idx):\n",
    "        loaded = np.load(str(self.dataset_path+'/batch_'+str(int(idx+1))))\n",
    "        array1 = loaded['array1_drm']\n",
    "        array2 = loaded['array2_pia']\n",
    "        array3 = loaded['array3_gtr']\n",
    "        array4 = loaded['array4_bss']\n",
    "        dec_input = tf.slice(tf.pad(array1,paddings=self.paddings),[0,0,0],array1.shape)\n",
    "        inputs = [\n",
    "            array2,array3,array4,\n",
    "            dec_input[:,:,:9],\n",
    "            dec_input[:,:,9:18],\n",
    "            dec_input[:,:,18:27]\n",
    "        ]\n",
    "        return inputs,array1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 27)\n",
      "(64, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "loaded = np.load('datasets/data_for_sequence/batch_2174.npz')\n",
    "array1 = loaded['array1_drm']\n",
    "array2 = loaded['array2_pia']\n",
    "array3 = loaded['array3_gtr']\n",
    "array4 = loaded['array4_bss']\n",
    "print(array1.shape)\n",
    "print(array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
